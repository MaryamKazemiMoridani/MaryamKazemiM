# -*- coding: utf-8 -*-
"""Estimation_of_Productivity.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VyeM24IfWEg6CZMkcIGtq4yJYvbYT1_5
"""

#####SECTION_One#####

#In this section I have tried to predict the productivity of a garment factory employees using the default choice of RandomForest Regressor

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
import matplotlib.pyplot as plt

# read in data and display first 5 rows
input_data = pd.read_csv('garments_worker_productivity.csv')
input_data.head(5)

#replace the blank cells of wip coloumn with -1
input_data['wip'] = input_data['wip'].replace(np.nan, -1)

#print the size of input_data
print('The shape of our input_data is:', input_data.shape)

#converting text data to binary vectors
input_data = pd.get_dummies(data = input_data, columns=['quarter', 'department', 'day'])
input_data.iloc[:,5:].head(5)

#print the size of input_data
print('Shape of input_data after one-hot encoding:', input_data.shape)

#store what we want to predict in the 'lables' array
#remove what we want to predict from input_data
input_data = input_data.drop('actual_productivity', axis = 1)
input_data = input_data.drop('targeted_productivity', axis = 1)
#saving the lables of input_data columns as a list
input_data.head(5)

#store what we want to predict in the 'lables' array
labels = np.array(input_data['actual_productivity'])
#saving the lables of input_data columns as a list
input_data_list = list(input_data.columns)
#convert to numpy array
input_data = np.array(input_data)

#split data to train_data & test_data with 75% to 25% ratio
train_data, test_data, train_labels, test_labels = train_test_split(input_data, labels, test_size = 0.25,
                                                                           random_state = 42)

#print the size of train_data & test_data
print('Training Features Shape:', train_data.shape)
print('Training Labels Shape:', train_labels.shape)
print('Testing Features Shape:', test_data.shape)
print('Testing Labels Shape:', test_labels.shape)

# saving the date coloumns of input_data in array dates 
dates = input_data[:, input_data_list.index('date')]
targeted_productivities = input_data[:, input_data_list.index('targeted_productivity')] 

# saving the actual_productivity column of input_data in true_data
true_data = pd.DataFrame(data = {'date': dates, 'actual_productivity': labels})

#saving date coulumns of test_data array in test_dates
test_dates = test_data[:, input_data_list.index('date')]
test_targeted = test_data[:, input_data_list.index('targeted_productivity')]

#removing date column of (input,train,test)_data and date lable of input_data_list
input_data = np.delete(input_data, 0, 1)
train_data = np.delete(train_data, 0, 1)
test_data = np.delete(test_data, 0, 1)
input_data_list.remove('date')
input_data_list.remove('targeted_productivity')

#defining a standard error for our calculation
baseline_predic = test_data[:, input_data_list.index('targeted_productivity')]

baseline_errors = abs(baseline_predic - test_labels)
print('Average baseline error: ', round(np.mean(baseline_errors), 3)*100, 'percent.')

#fit randomforestregressor on train data to find a pattern
rf = RandomForestRegressor(n_estimators= 1000, random_state=42)

rf.fit(train_data, train_labels);

#use predict function of randomforestregressor to make predictions about the actual_productivity
predictions = rf.predict(test_data)

#compute the error between our prediction and the reported values
errors = abs(predictions - test_labels)
          
print('Mean Absolute Error:', round(np.mean(errors), 2)*100 , 'percent.')

#saving the prediction of randomforest in predictions_data
predictions_data = pd.DataFrame(data = {'date': test_dates, 'prediction': predictions})

#ploting actual_productivity versus date (true_data was defined in cell #10)
plt.plot(true_data['date'], true_data['actual_productivity'], 'b-', label = 'actual_productivity')

#ploting predictions_data versus date
plt.plot(predictions_data['date'], predictions_data['prediction'], 'ro', label = 'prediction')
plt.xticks(rotation = '60'); 
plt.legend()

plt.xlabel('Date'); plt.ylabel('Productivity'); plt.title('Actual and Predicted Values');

# Calculate mean absolute percentage error
mape = 100 * (errors / test_labels)

# Calculate and display accuracy
accuracy = 100 - np.mean(mape)
print('Accuracy:', round(accuracy, 2), '%.')

#showing variables that have more effect on our calculations 
important_var = list(rf.feature_importances_)

feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(input_data_list, important_var)]

feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)

[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];

#plotting variables obtained in the previous cell (#18) in a chart
plt.style.use('fivethirtyeight')

x_values = list(range(len(important_var)))

plt.bar(x_values, important_var, orientation = 'vertical')

plt.xticks(x_values, input_data_list, rotation='vertical')

plt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');

#####SECTION_TWO#####

#In this section I have tried to tune the hyperparameters of the RandomForest Regressor to check if 
#the results can be improved. 

from pprint import pprint
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import GridSearchCV

#printing the parameters that are used in randomforestregressor
print('Parameters currently in use:\n')
pprint(rf.get_params())

#assigning a range of different values to RF parameters
# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 600, stop = 1400, num = 9)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(40, 110, num = 8)]
max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 3, 5]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 4]
# Method of selecting samples for training each tree
bootstrap = [True, False]
# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}
pprint(random_grid)

#Through RandomizedSearchCV, I try to find the best combination of the parameters above
rf = RandomForestRegressor()
rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)
rf_random.fit(train_data, train_labels)

#printing paremeters of the best combination
rf_random.best_params_

#defining a function to calculate the accuracy of the model in use
def evaluate(model, test_data, test_labels):
    predictions = model.predict(test_data)
    errors = abs(predictions - test_labels)
    mape = 100 * np.mean(errors / test_labels)
    accuracy = 100 - mape
    print('Model Performance')
    print('Average Error: {:0.4f} percent.'.format(np.mean(errors)))
    print('Accuracy = {:0.2f}%.'.format(accuracy))
    
    return accuracy

#Set the model used in cell #13 model as a base_model and calcute its accuracy
base_model = RandomForestRegressor(n_estimators= 1000, random_state=42)
base_model.fit(train_data, train_labels)
base_accuracy = evaluate(base_model, test_data, test_labels)

#Calcute the accuracy of the RandomizedSearchCV model(cell #23)
best_random = rf_random.best_estimator_
random_accuracy = evaluate(best_random, test_data, test_labels)

#showing the improvement of using random.best_estimator_
print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))

#defining a new grid of parameters, taking into account the result of cells #23 & 24 which has narrowed the choices
param_grid = {
    'bootstrap': [False],
    'max_depth': [20, 30, 40, 50, 60],
    'max_features': ['sqrt'],
    'min_samples_leaf': [1, 2, 3, 4],
    'min_samples_split': [2, 3, 4],
    'n_estimators': [500, 600, 700, 800, 900, 1000]
}
rf = RandomForestRegressor()
grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, 
                          cv = 3, n_jobs = -1, verbose = 2)

#using GridSearchCV function to find out the best combination of parameters
grid_search.fit(train_data, train_labels)

grid_search.best_params_

#finding the accuracy of the GridSearchCV
best_grid = grid_search.best_estimator_
grid_accuracy = evaluate(best_grid, test_data, test_labels)

#using "evaluate" function to find out the improvement
print('Improvement of {:0.2f}%.'.format( 100 * (grid_accuracy - base_accuracy) / base_accuracy))

#Since the perfomance of the Random Forest regrssor depends on the provided information in the input data, it seems 
#that the accuracy cannot be improved easily.
